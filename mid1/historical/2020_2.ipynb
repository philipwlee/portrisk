{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Mark Hendricks </div>\n",
    "\n",
    "<left>FINM 36700</left> \n",
    "<br>\n",
    "<left>Autumn 2020</left>\n",
    "\n",
    "<h2><center>Midterm #2 </center></h2>\n",
    "\n",
    "<center>Due on Wednesday, November 18, at 8:30pm.</center>\n",
    "\n",
    "<h3><center><span style=\"color:#00008B\">Solution</span></center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 True / False (25 pts)\n",
    "\n",
    "You are graded for your (brief) explanation.\n",
    "\n",
    "1.(5pts) Mean-variance optimization of inflation-adjusted returns will give a different answer than mean-variance optimization of nominal (not in inflation-adjusted) returns.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution: True.** The inflation adjusted return series will result in a different covariance matrix than nominal return series. If we are subtracting/adding a constant from a series, then covariance matrix wonâ€™t change, but inflation is not constant; it is a random variable which leads to a different covariance matrix, which will provide different mean-variance optimization. </span>\n",
    "\n",
    "2.(5pts) Based on our tests, the compensation for market beta is low relative to what theory\n",
    "implies.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution: True.** Theory (CAPM) implies that market beta can fully explain the mean excess return of the asset. However, all of our tests of the CAPM, (in both the homework and lecture notes,) show that the estimated premium for market beta is too low to explain the full asset premium. (Graphically, the ross-sectional fit has a high intercept and a relatively flat line.) </span>\n",
    "\n",
    "3.(5pts) An extreme momentum construction (using only the top-and-bottom deciles) leads to\n",
    "better performance, as expected by theory.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution: True or False.** The extreme construction was found to lead to higher mean returns, as expected in theory. But it also has more idiosyncratic risk and thus higher volatility, (also as expected in theory.) Whether this balance of higher return and higher risk is \"better\" is an open question.</span>\n",
    "\n",
    "4.(5pts) A long-short momentum strategy out-performs a long-only momentum strategy because\n",
    "it has a higher mean return due to earning positive returns on both sides, the longs and shorts.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution: True or False.** Shorting the biggest losers in the investment universe provides additional source of return for the strategy. However, our analysis showed that the mean return from doing this is lower than it is from putting all the weight of the portfolio into the long side of momentum. Accordingly, we saw that the long-only implementation in the homework had higher mean return. The problem with it was the high market correlation and higher volatility.\n",
    "</span>\n",
    "\n",
    "5.(5pts) In the case, DFA improves performance by combining their factor model with fundamental,\n",
    "firm-specific equity analysis.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution: False.** DFA is not making directional bets based on equity fundamentals, nor on macroeconomic analysis. Rather, DFA is largely relying on their top-down analysis via their Linear Factor Moldel which says size-beta and value-beta are associated with premium in the long run. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Short Answer (25 pts)\n",
    "\n",
    "1.(5pts) How did Harvard Management Co. ensure their tangency-portfolio weights would be\n",
    "realistic? What is a drawback of their method?\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution:** Harvard Management co. places bounds on the portfolio allocation rather than implementing whatever numbers come out of the MV optimization problem. The drawback to this approach is that the estimation becomes more complicated numerically and less conceptually clear. The solution may depend in fragile ways on the exact bounds used, which in Harvard's case seem to be arbitrary. </span>\n",
    "\n",
    "\n",
    "2.(5pts) Name one way in which Fama and French construct the factors that helps reduce cross-\n",
    "factor correlation.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution:** Fama and French construct the value factor to be relatively market neutral by making it a long-short portfolio, (high value and short growth.) They also reduce factor correlation to the size factor by constructing the factor separately for small and large stocks, then taking the average of the small-stock version of the factor and the large-stock version of the factor. We saw the momentum factor was built with both these considerations as well. </span>\n",
    "\n",
    "3.(5pts) One might say DFA is as focused on providing \"beta\" to investors as they are providing\n",
    "some \"alpha\". Isn't the point of a managed fund to provide \"alpha\"? Explain why DFA's\n",
    "product may be valuable even if only providing \"beta\".\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution:** DFA is providing beta in very efficient and at very low cost by doing block trading, managing liquidity and balancing various other execution variables. Thus, they outperform a simpler implementation of the Fama-French model. Whether this improvement is considered more efficient beta or the \"alpha\" of implementation, it is valuable for investors. Furthermore, for an investor with a CAPM model, their size and value \"betas\" will seem to provide \"alpha\".</span>\n",
    "\n",
    "4.(5pts) Given that we can test a Linear Factor Pricing Model using only the time-series regressions\n",
    "of the test securities, what is the use of the cross-sectional regression?\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution:** Cross-sectional regression allows more flexibility. The cross section allows us to consider that the factor premium may differ from the historic mean of the factor, (as is assumed in the time-series.) In the cross sectional regression, these factor premia are estimated directly as regression slopes.  </span>\n",
    "\n",
    "5.(5pts) Based on our analysis of ProShares, name a pro and a con of using a top-down replication\n",
    "strategy to get hedge-fund exposure.\n",
    "\n",
    "<span style=\"color:#00008B\"> **Solution:.** ***Pros*** can be: systematic, does not require specific knowledge of the strategies, more liquidity, more transparency, lower expenses. ***Cons:*** The approach cannot replicate the fund if its performance depends on specific market operations, illiquidity premium, market microstructure, trading speed, etc. Furthermore, replication will not work if the historic data is not representative of the ongoing fund strategy. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Allocation (30 pts)\n",
    "\n",
    "Here we use the provided excess return data on the 4 factors: MKT, SMB, HML, MOM, provided\n",
    "via \"ff_data.xlsx\" and \"momentum_data.xlsx\". (Remember that you do NOT need to use the risk-free\n",
    "rate data, \"RF\".)\n",
    "\n",
    "**1.(5pts) Calculate and display the following statistics for each of the 4 factors, and be sure to\n",
    "annualize them:**\n",
    " - mean\n",
    " - volatility\n",
    " - Sharpe ratio\n",
    " \n",
    "Also calculate and display the correlation matrix of the 4 factors. (Don't try to annualize the\n",
    "correlation matrix.)\n",
    "\n",
    "**2.(5pts) Using data through Dec 2010, calculate and display the weights of the tangency portfolio\n",
    "based on these 4 assets.**\n",
    "\n",
    "**3.(5pts) Calculate and display the IN-SAMPLE annualized mean, vol, and Sharpe ratio of the\n",
    "tangency portfolio. (i.e. based on the return data through Dec 2010.)**\n",
    "\n",
    "**4.(5pts) Calculate and display the OUT-OF-SAMPLE annualized mean, vol, and Sharpe ratio.\n",
    "(i.e. use the previously calculated tangency portfolio as it performs from Jan 2011 through the\n",
    "end of the sample.)**\n",
    "\n",
    "**5.(10pts) Name two reasons that mean-variance optimization is fragile, in the sense that the classic\n",
    "solution does not perform well out-of-sample.**\n",
    "\n",
    "Are those issues particularly impactful for this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "mom_df = pd.read_excel('momentum_data.xlsx', \"Momentum Factor\")\n",
    "mom_df.set_index('date', inplace=True)\n",
    "\n",
    "factors_df = pd.read_excel('ff_data.xlsx', \"FACTORS\")\n",
    "factors_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_factor = factors_df.join(mom_df)\n",
    "final_factor = final_factor[['MKT', 'SMB', 'HML', 'MOM']]\n",
    "final_factor = final_factor.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058f\" ><caption>Solution Table 1: mean, volatility and Sharpe ratio of each asset (Annualized)</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean</th>        <th class=\"col_heading level0 col1\" >volatility</th>        <th class=\"col_heading level0 col2\" >Sharpe ratio</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058flevel0_row0\" class=\"row_heading level0 row0\" >MKT</th>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow0_col0\" class=\"data row0 col0\" >0.080448</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow0_col1\" class=\"data row0 col1\" >0.185694</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow0_col2\" class=\"data row0 col2\" >0.43323</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058flevel0_row1\" class=\"row_heading level0 row1\" >SMB</th>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow1_col0\" class=\"data row1 col0\" >0.0233488</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow1_col1\" class=\"data row1 col1\" >0.110249</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow1_col2\" class=\"data row1 col2\" >0.211781</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058flevel0_row2\" class=\"row_heading level0 row2\" >HML</th>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow2_col0\" class=\"data row2 col0\" >0.0389947</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow2_col1\" class=\"data row2 col1\" >0.121697</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow2_col2\" class=\"data row2 col2\" >0.320423</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058flevel0_row3\" class=\"row_heading level0 row3\" >MOM</th>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow3_col0\" class=\"data row3 col0\" >0.0795705</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow3_col1\" class=\"data row3 col1\" >0.162635</td>\n",
       "                        <td id=\"T_8e7bec42_31d0_11eb_aabd_a8667f16058frow3_col2\" class=\"data row3 col2\" >0.489259</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x127d80b10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3.1\n",
    "def q3(data):\n",
    "    means = data.mean()*12\n",
    "    stds = data.std()*np.sqrt(12)\n",
    "    SR = means / stds\n",
    "    df = pd.DataFrame()\n",
    "    df['mean'] = means\n",
    "    df['volatility'] = stds\n",
    "    df['Sharpe ratio'] = SR\n",
    "    return df\n",
    "q3_df = q3(final_factor)\n",
    "q3_df.style.set_caption('Solution Table 1: mean, volatility and Sharpe ratio of each asset (Annualized)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MKT</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>MOM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>MKT</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>-0.3405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SMB</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>-0.1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HML</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.4114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MOM</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1548</td>\n",
       "      <td>-0.4114</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MKT     SMB     HML     MOM\n",
       "MKT  1.0000  0.3235  0.2356 -0.3405\n",
       "SMB  0.3235  1.0000  0.1264 -0.1548\n",
       "HML  0.2356  0.1264  1.0000 -0.4114\n",
       "MOM -0.3405 -0.1548 -0.4114  1.0000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_factor.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check sum of weights: 1.000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058f\" ><caption>Solution Table 2: weights of the tangency portfolio</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Weights of tangency portfolio</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058flevel0_row0\" class=\"row_heading level0 row0\" >MKT</th>\n",
       "                        <td id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058frow0_col0\" class=\"data row0 col0\" >0.178803</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058flevel0_row1\" class=\"row_heading level0 row1\" >SMB</th>\n",
       "                        <td id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058frow1_col0\" class=\"data row1 col0\" >0.087371</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058flevel0_row2\" class=\"row_heading level0 row2\" >HML</th>\n",
       "                        <td id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058frow2_col0\" class=\"data row2 col0\" >0.351112</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058flevel0_row3\" class=\"row_heading level0 row3\" >MOM</th>\n",
       "                        <td id=\"T_97d6370c_31d0_11eb_aabd_a8667f16058frow3_col0\" class=\"data row3 col0\" >0.382714</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10e207d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.2\n",
    "factor_dec = final_factor.loc[:'Dec-2010']\n",
    "from numpy.linalg import inv\n",
    "\n",
    "mu_tilde = factor_dec.mean() * 12\n",
    "std = factor_dec.std() * np.sqrt(12)\n",
    "corr = factor_dec.corr()\n",
    "Sigma = factor_dec.cov() * 12\n",
    "Sigma_inv = inv(Sigma)\n",
    "N = Sigma.shape[0]\n",
    "\n",
    "weights_t = Sigma_inv @ mu_tilde / (np.ones(N) @ Sigma_inv @ mu_tilde)\n",
    "print('Check sum of weights: {:.3f}'.format(weights_t.sum()))\n",
    "\n",
    "q3_2_df = pd.DataFrame(weights_t, index=factor_dec.columns, columns=['Weights of tangency portfolio'])\n",
    "display(q3_2_df.style.set_caption('Solution Table 2: weights of the tangency portfolio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_9cc740d0_31d0_11eb_aabd_a8667f16058f\" ><caption>Solution Table 3: mean, vol and Sharpe ratio of the tangency portfolio</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean</th>        <th class=\"col_heading level0 col1\" >vol</th>        <th class=\"col_heading level0 col2\" >Sharpe ratio</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_9cc740d0_31d0_11eb_aabd_a8667f16058flevel0_row0\" class=\"row_heading level0 row0\" >Tangency portfolio</th>\n",
       "                        <td id=\"T_9cc740d0_31d0_11eb_aabd_a8667f16058frow0_col0\" class=\"data row0 col0\" >0.0653632</td>\n",
       "                        <td id=\"T_9cc740d0_31d0_11eb_aabd_a8667f16058frow0_col1\" class=\"data row0 col1\" >0.0658182</td>\n",
       "                        <td id=\"T_9cc740d0_31d0_11eb_aabd_a8667f16058frow0_col2\" class=\"data row0 col2\" >0.993087</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1289f3b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.3\n",
    "tan_portf = factor_dec @ weights_t\n",
    "mean = tan_portf.mean()*12\n",
    "vol = tan_portf.std() * np.sqrt(12)\n",
    "SR = mean / vol\n",
    "q3_3_df = pd.DataFrame(index=['Tangency portfolio'])\n",
    "q3_3_df.loc['Tangency portfolio', 'mean'] = mean\n",
    "q3_3_df.loc['Tangency portfolio', 'vol'] = vol\n",
    "q3_3_df.loc['Tangency portfolio', 'Sharpe ratio'] = SR\n",
    "display(q3_3_df.style.set_caption('Solution Table 3: mean, vol and Sharpe ratio of the tangency portfolio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_a3a78392_31d0_11eb_aabd_a8667f16058f\" ><caption>Solution Table 4: mean, vol and Sharpe ratio of the tangency portfolio</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean</th>        <th class=\"col_heading level0 col1\" >vol</th>        <th class=\"col_heading level0 col2\" >Sharpe ratio</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_a3a78392_31d0_11eb_aabd_a8667f16058flevel0_row0\" class=\"row_heading level0 row0\" >Tangency portfolio</th>\n",
       "                        <td id=\"T_a3a78392_31d0_11eb_aabd_a8667f16058frow0_col0\" class=\"data row0 col0\" >0.018905</td>\n",
       "                        <td id=\"T_a3a78392_31d0_11eb_aabd_a8667f16058frow0_col1\" class=\"data row0 col1\" >0.0432655</td>\n",
       "                        <td id=\"T_a3a78392_31d0_11eb_aabd_a8667f16058frow0_col2\" class=\"data row0 col2\" >0.436954</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12811f910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.4\n",
    "tan_portf = final_factor.loc['Jan-2011':]@ weights_t\n",
    "mean = tan_portf.mean()*12\n",
    "vol = tan_portf.std() * np.sqrt(12)\n",
    "SR = mean / vol\n",
    "q3_4_df = pd.DataFrame(index=['Tangency portfolio'])\n",
    "q3_4_df.loc['Tangency portfolio', 'mean'] = mean\n",
    "q3_4_df.loc['Tangency portfolio', 'vol'] = vol\n",
    "q3_4_df.loc['Tangency portfolio', 'Sharpe ratio'] = SR\n",
    "display(q3_4_df.style.set_caption('Solution Table 4: mean, vol and Sharpe ratio of the tangency portfolio'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#00008B\"> **Solution:** MVO is fragile for two reason:</span>\n",
    "\n",
    "   <span style=\"color:#00008B\"> 1. MV Optimization is sensitive to mean returns, which are not estimated precisely from historic data. </span>\n",
    "    \n",
    "   <span style=\"color:#00008B\"> 2. MV Optimization is extremely sensitive to the covariance matrix, which changes over time. </span>\n",
    "   \n",
    "<span style=\"color:#00008B\"> In this example, these issues seem to matter a lot, as the Sharpe Ratio drops from .99 to .44 when we move to the out-of-sample period. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Hedging & Replication (40pts)\n",
    "\n",
    "Continue to use the same data from the previous problem. That is, the excess returns on the 4 factors.\n",
    "(Remember they were provided to you as excess returns, so no need to use the risk-free-rate data\n",
    "provided along with those factors.)\n",
    "\n",
    "**1.We want to build a new factor, SMB*, which is just SMB but with exposure to MKT completely hedged out.**\n",
    "\n",
    "   Regress SMB on MKT.\n",
    "    \n",
    "   - Include an intercept.\n",
    "        \n",
    "   - Use the full sample of data, based on the overlapping dates in the Fama-French and momenum data. (i.e. The same data from the previous problem, with dates 1/31/27 to 8/31/20.)\n",
    "      \n",
    "      \n",
    "  (a) (10pts) Report alpha, beta, and the r-squared of the regression.\n",
    "    \n",
    "  (b) (5pts) Report the Sharpe Ratio of the new factor, $SMB^*$.\n",
    "    \n",
    "  (c) (5pts) In what specific measure has $SMB^*$ optimally hedge the market?\n",
    "  \n",
    "**2.Suppose we want to build a new factor,$MOM^{**}$, which is a replication of MOM, using MKT,\n",
    "SMB, and HML. Regress MOM on MKT, SMB, and HML.**\n",
    "\n",
    "- Once again, include an intercept.\n",
    "- Once again, use the full data sample.\n",
    "\n",
    "    (a) (10pts) Report alpha, beta, and the r-squared of the regression.\n",
    "    \n",
    "    (b) (5pts) Report the correlation between MOM and $MOM^{**}$.\n",
    "    \n",
    "\n",
    "**3.(5pts) In what statistical sense would our hedge, $SMB^*$, or our replication, $MOM^{**}$, be worse had we not included intercepts in the regressions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'momentum_data.xlsx'\n",
    "path2 = 'ff_data.xlsx'\n",
    "df_factor = pd.read_excel(path, sheet_name='Momentum Factor').set_index('date')\n",
    "temp = pd.read_excel(path2, sheet_name='FACTORS').set_index('date')\n",
    "df_factor = df_factor.join(temp)\n",
    "# display(df_factor.head(1))\n",
    "\n",
    "temp = pd.read_excel(path2, sheet_name='PORTFOLIOS').set_index('date')\n",
    "temp.columns = temp.columns.str.strip()\n",
    "ind_portfolios_names = temp.columns.values\n",
    "df_industries = df_factor.join(temp)\n",
    "# display(df_industries.head(1))\n",
    "\n",
    "temp = pd.read_excel(path, sheet_name='Momentum Deciles').set_index('date')\n",
    "temp.columns = temp.columns.str.strip()\n",
    "df_momentum = df_factor.join(temp)\n",
    "# display(df_momentum.head(1))\n",
    "\n",
    "temp = pd.read_excel(path, sheet_name='Momentum by Size').set_index('date')\n",
    "temp.columns = temp.columns.str.strip()\n",
    "mom_size_portfolio_names = temp.columns\n",
    "df_mom_size = df_factor.join(temp)\n",
    "# display(df_mom_size.head(1))\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4.1(a)\n",
      "Alpha = 0.001\n",
      "Beta = 0.192\n",
      "R-squared = 0.104\n",
      "\n",
      "Q4.1(b)\n",
      "\n",
      "SR of the new SMB factor (not annualized) = 0.022\n",
      "SR of the new SMB factor (annualized) = 0.076\n",
      "\n",
      "Q4.1(c)\n",
      "\n",
      "New SMB beta when regressed on the MKT = -0.000\n",
      "Corr(SMB new, MKT) = -0.000\n",
      "\n",
      "New SMB optimally hedges the market in terms of EXCESS RETURN as correlation as well as beta are zeros.\n"
     ]
    }
   ],
   "source": [
    "# Q4.1\n",
    "rhs = sm.add_constant(df_mom_size['MKT'])\n",
    "lhs = df_mom_size['SMB']\n",
    "reg = sm.OLS(lhs, rhs, missing='drop').fit()\n",
    "\n",
    "# (a)\n",
    "alpha = reg.params['const']\n",
    "beta = reg.params['MKT']\n",
    "r2 = reg.rsquared_adj\n",
    "\n",
    "print('Q4.1(a)')\n",
    "print('Alpha = {:.3f}'.format(alpha))\n",
    "print('Beta = {:.3f}'.format(beta))\n",
    "print('R-squared = {:.3f}'.format(r2))\n",
    "\n",
    "# (b)\n",
    "smb_new = df_mom_size['SMB'] - beta*df_mom_size['MKT']\n",
    "\n",
    "SR_smb_new_m = (smb_new.mean()) / (smb_new.std())\n",
    "SR_smb_new_a = (smb_new.mean()*12) / (smb_new.std()*np.sqrt(12))\n",
    "print('\\nQ4.1(b)\\n\\nSR of the new SMB factor (not annualized) = {:.3f}'.format(SR_smb_new_m))\n",
    "print('SR of the new SMB factor (annualized) = {:.3f}'.format(SR_smb_new_a))\n",
    "\n",
    "# (c)\n",
    "print('\\nQ4.1(c)\\n')\n",
    "corr_smb_new_mkr = smb_new.corr(df_mom_size['MKT'])\n",
    "rhs = sm.add_constant(df_mom_size['MKT'])\n",
    "lhs = smb_new\n",
    "reg = sm.OLS(lhs, rhs, missing='drop').fit()\n",
    "print('New SMB beta when regressed on the MKT = {:.3f}'.format(reg.params['MKT']))\n",
    "print('Corr(SMB new, MKT) = {:.3f}'.format(corr_smb_new_mkr))\n",
    "print('\\nNew SMB optimally hedges the market in terms of EXCESS RETURN as correlation as well as beta are zeros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4.2(a)\n",
      "\n",
      "Alpha = 0.010\n",
      "Beta (MKT) = -0.218\n",
      "Beta (SMB) = -0.045\n",
      "Beta (HML) = -0.466\n",
      "R-squared = 0.231\n",
      "\n",
      "Q4.2(b)\n",
      "\n",
      "Corr(MOM, MOM_new) = 0.480\n",
      "\n",
      "Q4.2(c)\n"
     ]
    }
   ],
   "source": [
    "# Q4.2\n",
    "rhs = sm.add_constant(df_mom_size[['MKT','SMB', 'HML']])\n",
    "lhs = df_mom_size['MOM']\n",
    "reg = sm.OLS(lhs, rhs, missing='drop').fit()\n",
    "\n",
    "# (a)\n",
    "alpha = reg.params['const']\n",
    "beta_mkt, beta_smb, beta_hml = reg.params['MKT'], reg.params['SMB'], reg.params['HML']\n",
    "r2 = reg.rsquared_adj\n",
    "\n",
    "print('Q4.2(a)\\n')\n",
    "print('Alpha = {:.3f}'.format(alpha))\n",
    "print('Beta (MKT) = {:.3f}'.format(beta_mkt))\n",
    "print('Beta (SMB) = {:.3f}'.format(beta_smb))\n",
    "print('Beta (HML) = {:.3f}'.format(beta_hml))\n",
    "print('R-squared = {:.3f}'.format(r2))\n",
    "\n",
    "# (b)\n",
    "corr_mom = np.sqrt(r2)\n",
    "\n",
    "print('\\nQ4.2(b)\\n')\n",
    "print('Corr(MOM, MOM_new) = {:.3f}'.format(corr_mom))\n",
    "\n",
    "# (c)\n",
    "print('\\nQ4.2(c)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#00008B\"> **Solution:** If we do not include an intercept, we would try to capture both variation and mean with our beta(s). Mean(s) are usually noisy, and by not including an intercept we may get a bad ot-of-sample performance.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
